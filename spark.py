# -*- coding: utf-8 -*-
"""dsci551_Project_SparkMLib

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1weIr2pvXR652wBtnfK3Smjz8qp19VCVy

# ML Implementation via **Spark MLib**
"""
import pandas as pd
import numpy as np
import streamlit as st

import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

from statsmodels.tsa.stattools import adfuller

from math import ceil

from pyspark.sql.window import Window
from pyspark.sql.functions import col, asc, desc, to_timestamp,\
                                  unix_timestamp, from_unixtime
from pyspark.sql.types import StructType, StructField, LongType
import pyspark.sql.functions as func
from pyspark.sql import SparkSession, SQLContext
from pyspark import SparkConf
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator

from dataload import uploadData,downloadData,getCollections

conf = SparkConf()
spark = SparkSession.builder.appName("TimeSeries").master("local").config(conf=conf).getOrCreate()

"""## Functions"""

########The Methods used in thefunction are defined here
def app():

  def checkStationarity(timeseries):
    """
    #Perform dickey fuller test
    """
    dftest = adfuller(timeseries, autolag = 'AIC')
    #print(dftest[0:4])
    dfoutput = pd.Series(dftest[0:4], index = ['Test Statitic','p-value','#Lags Used','Number of Observations Used'])
    #print(dftest[4])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
        
    if dftest[1]>0.05:
        print("Data is Non Stationary. Has Relation with time")
    return dfoutput

  #-----------------------------------------------------------------------------------

  def createLag(df, colName, numLags):
    """
    Create a new column with the specified lag
    """
    colNameList = []
    df = df.withColumn("Series", func.lit('UNIVARIATE'))
    windowSpec = Window.orderBy('Series')
    for i in range(numLags):
        newCol = colName+"_LagBy_"+str(i+1)
        colNameList.append(newCol)
        df = df.withColumn(newCol, func.lag(df['CLOSE'],i+1).over(windowSpec))
    df.drop("Series")
    return df, colNameList
    
  #-----------------------------------------------------------------------------------

  def findMovingAverage(df, colName, lagLength):
    """
    Creates a Moving average column over a specified lag window
    """
    newcolName = colName + "_" + str(lagLength) + "_" + "MovingAvg"
    windowSpec = Window.rowsBetween(-lagLength, 0)
    df = df.withColumn(newcolName, func.avg(df[colName]).over(windowSpec))
    return df, newcolName
    
  #-----------------------------------------------------------------------------------

  def findTrend(df,colName,lagLength):
    """
    Generates a new column to indicate whether the trend is positive 
    or negative between two subsequent columns
    Positive changes are indicated by a value one and negative by -1
    """
    colNameList = []
    df = df.withColumn("Series", func.lit('UNIVARIATE'))
    windowSpec = Window.orderBy('Series')
    for i in range(lagLength):
        newCol = colName+"_Lag_" + str(i+1) + "Sign"
        colNameList.append(newCol)
        df = df.withColumn(newCol, func.signum(df['CLOSE'] - func.lag(df['CLOSE'],i+1)\
                           .over(windowSpec)))
    df = df.drop("Series")
    return df, colNameList
    
  #-----------------------------------------------------------------------------------
  def findDifference(df,colName):
    """
    Perform time differencing to detrend the data
    """
    newColName = "diff_" + colName
    lagWindow = Window.rowsBetween(-1,0)
    df = df.withColumn(newColName, df[colName] - \
                       func.first(df[colName]).over(lagWindow))
    return df, newColName

  #-----------------------------------------------------------------------------------

  def TimeSeriesSplit(df, splitratio, sparksession):
    """
    Split the data into train and test set
    """
    newSchema = StructType(df.schema.fields + [StructField("Row Number", LongType(), False)])
    newRDD = df.rdd.zipWithIndex().map(lambda x: list(x[0]) + [x[1]])
    df2 = sparksession.createDataFrame(newRDD, newSchema)
    total_rows = df2.count()
    splitFraction = int(total_rows*splitratio)
    df_train = df2.where(df2["Row Number"] >= 0).where(df2["Row Number"]<=splitFraction)
    df_test = df2.where(df2["Row Number"] > splitFraction)
    return df_train, df_test


  #-----------------------------------------------------------------------------------

  def Predict(i, df1, df2, timeSeriesCol, predictionCol, joinCol):
      
      # this converts differenced predictions to raw predictions
      dZCol = 'DeltaZ'+str(i) 
      f_strCol = 'forecast_'+str(i)+'day'
      df = df1.join(df2, [joinCol], how='inner').orderBy(asc('Date'))
      df = df.withColumnRenamed(predictionCol, dZCol)
      df = df.withColumn(f_strCol, col(dZCol)+col(timeSeriesCol))
      return df

  """## Forecast Function"""

  def Forecast(df, num_days, num_lags, timeseries_colName,split_ratio, regressor , sparksession):
    leadWindow = Window.rowsBetween(0, num_days)

    #Create labels for machine learning
    df = df.withColumn("label", func.last(df[timeseries_colName]).over(leadWindow))
    features = [timeseries_colName]

    #Autoregression feature
    df, colList = createLag(df, timeseries_colName, num_lags)
    features = features + colList

    ###Vector Assembler
    #A feature transformer that merges multiple columns into a vector column
    df = df.dropna()
    va = VectorAssembler().setInputCols(features).setOutputCol("features")
    df_m = va.transform(df)

    #Split data into train and test
    df_train, df_test = TimeSeriesSplit(df_m, split_ratio, sparksession)

    ###Decesion tree regressor
    if (regressor  == "DecisionTreeRegression"):
      decisiontree = DecisionTreeRegressor(featuresCol = "features", labelCol = "label", maxDepth = 5)
      model = decisiontree.fit(df_train)
      predictions_decisiontree_test = model.transform(df_test)
      predictions_decisiontree_train = model.transform(df_train)

      ###RMSE used as evaluation metric
      evaluator = RegressionEvaluator(predictionCol="prediction",\
                                          labelCol="label", metricName ="rmse")
      RMSE_dr_test = evaluator.evaluate(predictions_decisiontree_test)
      RMSE_dr_train = evaluator.evaluate(predictions_decisiontree_train)
      return (df_test, df_train, predictions_decisiontree_test, predictions_decisiontree_train,\
              RMSE_dr_test, RMSE_dr_train)
      
    ###Linear Regressor
    if (regressor == 'RandomForestRegression'):
      rfr = RandomForestRegressor(featuresCol="features",labelCol="label",\
                                      maxDepth = 5,subsamplingRate = 0.8,\
                                      )
      model = rfr.fit(df_train)
      predictions_rfr_test = model.transform(df_test)
      predictions_rfr_train = model.transform(df_train)
      
      # RMSE is used as evaluation metric
      evaluator = RegressionEvaluator(predictionCol="prediction",labelCol="label",\
                                      metricName ="rmse")
      RMSE_rfr_test= evaluator.evaluate(predictions_rfr_test)
      RMSE_rfr_train = evaluator.evaluate(predictions_rfr_train)

      return (df_test, df_train, predictions_rfr_test, predictions_rfr_train,\
              RMSE_rfr_test, RMSE_rfr_train)
    
    # LINEAR REGRESSOR
    if(regressor == 'LinearRegression'):
      lr = LinearRegression(featuresCol = "features", labelCol="label", \
                            maxIter = 100, regParam = 0.4, \
                            elasticNetParam = 0.1)
      model = lr.fit(df_train)
      predictions_lr_test = model.transform(df_test)
      predictions_lr_train = model.transform(df_train)
      
      # RMSE is used as evaluation metric
      evaluator = RegressionEvaluator(predictionCol="prediction",\
                                      labelCol="label",\
                                      metricName ="rmse")
      RMSE_lr_test= evaluator.evaluate(predictions_lr_test)
      RMSE_lr_train = evaluator.evaluate(predictions_lr_train)
      return (df_test, df_train, \
              predictions_lr_test, predictions_lr_train,\
              RMSE_lr_test, RMSE_lr_train)
      
      # GRADIENT BOOSTING TREE REGRESSOR
      if(regressor == 'GBTRegression'):
        gbt = GBTRegressor(featuresCol="features",\
                            labelCol="label",\
                            maxDepth=5,\
                            subsamplingRate=0.8)
        
        model = gbt.fit(df_train)
        predictions_gbt_test = model.transform(df_test)
        predictions_gbt_train = model.transform(df_train)
        
        # RMSE is used as evaluation metric
        evaluator = RegressionEvaluator(predictionCol="prediction",\
                                        labelCol="label",\
                                        metricName ="rmse")
        RMSE_gbt_test= evaluator.evaluate(predictions_gbt_test)
        RMSE_gbt_train = evaluator.evaluate(predictions_gbt_train)
        return (df_test, df_train, \
                predictions_gbt_test, predictions_gbt_train,\
                RMSE_gbt_test, RMSE_gbt_train)



  """## Save Predictions"""

  def SavePredictions(df, timeSeriesCol,regressionType,forecast_days, splitRatio, feature_nLags,sparksession):
      
      # this is the main function which calls forecast and predict
      # this saves predictions in csv files
      
      #Differencing data to remove non-stationarity
      #diff_timeSeriesCol = "Diff_"+timeSeriesCol
      df,diff_timeSeriesCol = findDifference(df, timeSeriesCol)
      
      RMSE_test = {}
      RMSE_train = {}
      
      #Forecasting and Undifferencing the data
      for i in range(1, forecast_days+1):
      
          # training with Spark's ML algorithms    
          df_test, df_train, predictions_test, predictions_train,\
          RMSE_ts, RMSE_tr = \
          Forecast(df.select("Date",timeSeriesCol,diff_timeSeriesCol),i, feature_nLags, \
                         diff_timeSeriesCol,splitRatio, regressionType, sparksession)
          RMSE_test.update({'forecast_'+str(i)+'day':RMSE_ts})
          RMSE_train.update({'forecast_'+str(i)+'day':RMSE_tr})
          #predictions for training data            
          if(i == 1):
              
              #saving the 1-day forecast as separate column
              corr_predict_train = Predict(i,df_train.select("Row Number","Date",timeSeriesCol),\
                                                           predictions_train.select("Row Number","prediction"),
                                                           timeSeriesCol,"prediction","Row Number")
              
              corr_predict_test = Predict(i, df_test.select("Row Number","Date",timeSeriesCol),\
                                                          predictions_test.select("Row Number","prediction"),
                                                          timeSeriesCol,"prediction","Row Number") 
          else:
              # saving each subsequent forecast as separate column
              strCol_prev= "forecast_" + str(i-1) + "day"
              corr_predict_train = Predict(i, corr_predict_train,predictions_train.select("Row Number","prediction"),\
                                                           strCol_prev,"prediction","Row Number")
              corr_predict_test = Predict(i,corr_predict_test,predictions_test.select("Row Number","prediction"),\
                                                          strCol_prev,"prediction","Row Number")
          # saving actual labels as separate columns
          LeadWindow = Window.rowsBetween(0, i)    
          a_strCol = "actual_"+str(i)+"day"
          corr_predict_test = corr_predict_test.withColumn(a_strCol, \
                                         func.last(corr_predict_test[timeSeriesCol])\
                                                .over(LeadWindow))
          corr_predict_train = corr_predict_train.withColumn(a_strCol, \
                                           func.last(corr_predict_test[timeSeriesCol])\
                                                  .over(LeadWindow))
      print("RMSE for train data:\n")
      print(RMSE_train)
      print("RMSE for test data:\n")
      print(RMSE_test)
      return corr_predict_test, corr_predict_train



  """## Read Data and convert to Spark Data frame 
  Add the data extraction from Firebase here for the new db
  """
  st.markdown("### Spark Data Analysis for analysis") 
  st.write("\n")
  ticker_list = getCollections('StockData')
  ticker_list.insert(0, 'None')
  tickerSymbol = st.sidebar.selectbox('Stock ticker', ticker_list)

  if tickerSymbol == 'None':
      st.info("Select a Stock ticker to proceed with")
  else:
      df = downloadData('StockData',tickerSymbol)
      #df = df[['Date','OPEN','HIGH','LOW','VOLUME','CLOSE']]
      st.header(f"**{tickerSymbol} data**")
      st.dataframe(df.head())

  # apple_data = pd.read_csv('../data/AAPL.csv') #Change this based on user selection of ticker. Use download Data
  # apple_data.set_index(apple_data['Date'],inplace= True)
  # apple_data['Date'] = pd.to_datetime(apple_data['Date'])

  # apple_df = spark.createDataFrame(apple_data)
  # apple_df_pd = apple_df.toPandas()

  #streamlit supports pandas only
      spark_df = spark.createDataFrame(df)
      #st.write(spark_df.show(10))

      ##

      """## Check Stationarity"""

      output = checkStationarity(df['CLOSE'])
      # st.write(output)



      """## Setup ML features and do prediction

      **The below variables can be used to take input from users and hence should be available in the UI**
      """

      # setting up parameters for simulation
      timeSeriesCol = st.selectbox('Select Time Series Column',('CLOSE', 'OPEN'))
      #timeSeriesCol = "CLOSE" #Select box
      regressionType = st.selectbox('Select Regression Type',('LinearRegression', 'RandomForestRegression', 'DecisionTreeRegression'))
      #regressionType = "LinearRegression" #A select box
      forecast_days = st.slider('Forecast Days', 1, 10, 6)
      #forecast_days = 5 #Slider
      num_lags = st.slider('Number of Lags', 1, 10, 3)
      #num_lags = 3 #Slider
      splitRatio = 0.7 # A slider

      predict_test, predict_train = SavePredictions(spark_df,timeSeriesCol, regressionType, forecast_days, splitRatio, num_lags, spark)

      #The results can be displayed


      st.write(predict_test.toPandas())

      predict_test_pandas = predict_test.toPandas()
      predict_test_pandas = predict_test_pandas.drop("Row Number", axis = 1).set_index("Date")

      timestamp = datetime.today().strftime("%Y%m%d")
      uploadData('Predictions', tickerSymbol+''+regressionType+""+timestamp, predict_test_pandas)

      sns.lineplot(data = predict_test_pandas, x = predict_test_pandas.index, y = "forecast_1day", markers = True)
      sns.lineplot(data = predict_test_pandas, x = predict_test_pandas.index, y = "actual_1day", markers = 1)
      st.pyplot(plt)

      forecastList = ["forecast_" +str(i)+ "day" for i in range(1,forecast_days+1)]
      actualList = ["actual_" +str(i)+ "day" for i in range(1,forecast_days+1)]
      colzipped = zip(forecastList, actualList)

      nrow = ceil(forecast_days/2)
      fig, axes = plt.subplots(ncols = 2,  nrows = nrow)
      for i, cols in enumerate(colzipped):
        if i%2 == 0:
          j = 0
        else:
          j = 1
        sns.lineplot(data = predict_test_pandas, x = predict_test_pandas.index, y=cols[0], ax = axes[i%nrow,j ])
        sns.lineplot(data = predict_test_pandas, x = predict_test_pandas.index, y=cols[1], ax = axes[i%nrow,j])
      st.pyplot(fig)
        







